{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artificial-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from surrogate_model import Aleatoric_NN, Epistemic_NN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from offloader import Offloader, OffloadVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-silence",
   "metadata": {},
   "source": [
    "## Active Learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elder-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = 2000\n",
    "n_test = 2000\n",
    "k = [500, 1000, 2000, 4000, 8000, 16000, 32000]\n",
    "m=4\n",
    "iterations = 10\n",
    "active_learning_mses = []\n",
    "baseline_mses = []\n",
    "baseline_ns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "banned-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = ([-3,3],[-2,2])\n",
    "\n",
    "def six_hump_camel_function(X):\n",
    "    x = X[:,0]\n",
    "    y = X[:,1]\n",
    "\n",
    "    x2 = np.power(x,2)\n",
    "    x4 = np.power(x,4)\n",
    "    y2 = np.power(y,2)\n",
    "\n",
    "    return (4.0 - 2.1 * x2 + (x4 / 3.0)) * x2 + x*y + (-4.0 + 4.0 * y2) * y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "veterinary-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 1200\n",
    "X1 = np.linspace(dom[0][0], dom[0][1], grid)\n",
    "X2 = np.linspace(dom[1][0], dom[1][1], grid)\n",
    "x1, x2 = np.meshgrid(X1, X2)\n",
    "X = np.hstack((x1.reshape(grid*grid,1),x2.reshape(grid*grid,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "australian-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "test_indices = np.random.randint(0, (grid*grid), size = n_test)\n",
    "testset_X = X[test_indices,:]\n",
    "X = np.delete(X, test_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "light-supervision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "(1438003, 2)\n"
     ]
    }
   ],
   "source": [
    "print(testset_X.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fundamental-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_y = six_hump_camel_function(testset_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adult-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_set = X[np.random.randint(0,X.shape[0],n_init),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "substantial-plumbing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "going-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = six_hump_camel_function(init_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opened-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(task_folder, x, y):\n",
    "    res = {'x':x, 'y':y}\n",
    "    with open(os.path.join(task_folder,\"example\", \"new_content.json\"), \"w\") as f:\n",
    "        json.dump(res, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "frequent-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post(task_folder, x, y):\n",
    "    with open(os.path.join(task_folder,\"example\",\"new_new_content.json\"), \"r\") as f:\n",
    "        res = json.load(f)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "technological-audience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offloader connects to: http://offload.dt4si.nl/api/v1\n"
     ]
    }
   ],
   "source": [
    "offloader = Offloader(\"offload.dt4si.nl\", \"api/v1\", offload_folder=\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "played-magazine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bakkermpd\\venvs\\ERP_DT_SI\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offloading vector folder: tmp\\offload-xocf6\n",
      "Argo workflow name: vector-99898\n",
      "total: 1, pending: 1, running: 0, succeeded: 0, failed: 0, finished: False\n",
      "total: 1, pending: 0, running: 0, succeeded: 0, failed: 0, finished: True\n"
     ]
    },
    {
     "ename": "OffloadFailedException",
     "evalue": "The offloading failed, see argo.dt4si.nl, workflow: vector-99898",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOffloadFailedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3c49a3b0a7e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"example\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"new_new_content.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0my_new_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#get elements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvs\\ERP_DT_SI\\lib\\site-packages\\offloader\\offloadvector.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;31m# offload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# gather results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvs\\ERP_DT_SI\\lib\\site-packages\\offloader\\offloadvector.py\u001b[0m in \u001b[0;36m_offloads\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;34m\"resources\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_resources\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         }\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffload_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_delete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_auto_delete\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moffload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moffload\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mart\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_artifacts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venvs\\ERP_DT_SI\\lib\\site-packages\\offloader\\offload.py\u001b[0m in \u001b[0;36moffload_vector\u001b[1;34m(self, vector_config, files, auto_delete)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0moffload_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_offload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwf_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moffload_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'phase'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Succeeded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mOffloadFailedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The offloading failed, see argo.dt4si.nl, workflow: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwf_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mOffload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffload_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_delete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOffloadFailedException\u001b[0m: The offloading failed, see argo.dt4si.nl, workflow: vector-99898"
     ]
    }
   ],
   "source": [
    "for j in k:\n",
    "    total_n = 0\n",
    "    nn = Epistemic_NN()\n",
    "    start_set = copy.deepcopy(init_set)\n",
    "    y_new = copy.deepcopy(y)\n",
    "    nn.create_model(start_set)\n",
    "    nn.fit(start_set, y_new, epochs = 25)\n",
    "    for i in range(iterations):\n",
    "        new_set = X[np.random.randint(0,X.shape[0],m*j),:]\n",
    "        evaluation_m, evaluation_s = nn.predict(new_set)\n",
    "        #print(evaluation_s)\n",
    "        highest_error_indices = np.argpartition(evaluation_s, -j)[-j:]\n",
    "        \n",
    "        #rebuild dict for multiple datapoint\n",
    "        vec = [{'x':new_set[0,0], 'y':new_set[0,1]}]\n",
    "        off = OffloadVector(offloader, pre, post, \"ls && pip install numpy && python test_functions.py\", \"python:3\", vec, local=False, auto_delete=False)\n",
    "        off.add_file(\"example\", \"\")\n",
    "        off.add_file(\"test_functions.py\", \"\")\n",
    "        off.get_file(os.path.join(\"example\",\"new_new_content.json\"))\n",
    "\n",
    "        y_new_ = off.run() #get elements\n",
    "        print(y_new)\n",
    "        \n",
    "        #y_new_ = six_hump_camel_function(new_set[highest_error_indices,:])\n",
    "        print(f\"Mean prediction uncertainty: {np.mean(evaluation_s[highest_error_indices])}\")\n",
    "        #start_set = np.concatenate((start_set,  new_set[highest_error_indices]), axis = 0)\n",
    "        print(f\"trainset size: {start_set.shape[0]}\")\n",
    "        #y_new = np.concatenate((y_new, y_new_), axis=0)\n",
    "        nn.updateModel(new_set[highest_error_indices,:], y_new_, epochs = 20, verbose=0)\n",
    "        total_n+=j\n",
    "        print(f\"finished n={j}, iteration={i}\")\n",
    "    nn.model.save(f\"AL_{j}.h5\")\n",
    "        \n",
    "    test_m, test_s = nn.predict(testset_X)\n",
    "    active_learning_mses.append(mean_squared_error(testset_y, test_m))\n",
    "    print(f\"mse: {active_learning_mses}\")\n",
    "    baseline_ns.append(total_n)\n",
    "    print(f\"baseline_ns: {baseline_ns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-accordance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-parts",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ERP_DT_SI",
   "language": "python",
   "name": "erp_dt_si"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
